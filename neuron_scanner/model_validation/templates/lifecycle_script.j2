{# Lifecycle Script for SageMaker Notebook Instance Neuron Compilation #}
{# ===================================================================== #}
{#
ARCHITECTURE:
    This script runs on SageMaker Notebook Instance startup via lifecycle configuration.
    It downloads the model, runs Neuron compilation in a Docker container, and uploads results.

VARIABLES:
    model_s3_uri: S3 URI to the model.tar.gz file
    output_s3_uri: S3 URI prefix for output (result.json, compiled_model.pt)
    script_s3_uri: S3 URI to the compile_script.py
    input_shape_str: Comma-separated input shape (e.g., "1,512") or empty for auto-detect
    region: AWS region
    container_image: Full Neuron DLC image URI from public ECR
    neuron_sdk_version: Neuron SDK version (e.g., "2.27.1") extracted from container_image
#}
#!/bin/bash
set -e

# =============================================================================
# Environment Variables
# =============================================================================
export MODEL_S3_URI="{{ model_s3_uri }}"
export OUTPUT_S3_URI="{{ output_s3_uri }}"
export SCRIPT_S3_URI="{{ script_s3_uri }}"
export INPUT_SHAPE="{{ input_shape_str }}"

# =============================================================================
# Logging
# =============================================================================
echo "Starting Neuron compilation lifecycle script..."
echo "MODEL_S3_URI: $MODEL_S3_URI"
echo "OUTPUT_S3_URI: $OUTPUT_S3_URI"

# =============================================================================
# Setup Working Directories
# =============================================================================
mkdir -p /tmp/neuron-compile/model
mkdir -p /tmp/neuron-compile/output
chmod -R 777 /tmp/neuron-compile  # Ensure Docker has write access

# =============================================================================
# Download Model and Compile Script
# =============================================================================
aws s3 cp "$MODEL_S3_URI" /tmp/neuron-compile/model.tar.gz

cd /tmp/neuron-compile/model
tar -xzf ../model.tar.gz

aws s3 cp "$SCRIPT_S3_URI" /tmp/neuron-compile/compile_script.py

# =============================================================================
# Configure Docker to Use EBS Volume (for large training containers)
# =============================================================================
echo "Configuring Docker to use EBS volume..."

# Stop Docker daemon
sudo systemctl stop docker

# Create Docker directory on EBS volume (50-100 GB available)
sudo mkdir -p /home/ec2-user/SageMaker/docker

# Backup old Docker data if it exists
if [ -d /var/lib/docker ]; then
    sudo mv /var/lib/docker /var/lib/docker.backup 2>/dev/null || true
fi

# Configure Docker daemon to use EBS volume
sudo tee /etc/docker/daemon.json > /dev/null <<EOF
{
  "data-root": "/home/ec2-user/SageMaker/docker"
}
EOF

# Start Docker with new configuration
sudo systemctl start docker
sudo systemctl status docker --no-pager || true

echo "‚úÖ Docker configured to use EBS volume"
echo "Available space on EBS:"
df -h /home/ec2-user/SageMaker

# =============================================================================
# Run Compilation in Docker (AWS Neuron DLC)
# =============================================================================
echo "Pulling Neuron DLC image from public ECR..."
echo "Image: {{ container_image }}"
docker pull {{ container_image }}

echo "Running compilation inside Docker..."
# We map /tmp/neuron-compile (host) to /app (container)
# We install transformers/safetensors just in case the DLC version is old
docker run --rm \
    -v /tmp/neuron-compile:/app \
    -e INPUT_SHAPE="$INPUT_SHAPE" \
    {{ container_image }} \
    /bin/bash -c "pip install --upgrade transformers safetensors && cd /app && python3 compile_script.py --input-dir /app/model --output-dir /app/output" || {
    echo "ERROR: Docker compilation failed"
    # Create an error result so the validator knows it failed
    echo '{"status": "ERROR", "error": "Docker compilation failed. Check CloudWatch logs for details."}' > /tmp/neuron-compile/output/result.json
    aws s3 cp /tmp/neuron-compile/output/result.json "$OUTPUT_S3_URI/result.json"
    exit 1
}

echo "‚úÖ Docker compilation complete"

# =============================================================================
# Validate Model Load in Docker (Same Environment)
# =============================================================================
echo ""
echo "üß™ Testing model load in Docker container..."

# Discover Neuron devices on the host
echo "Discovering Neuron devices..."
NEURON_DEVICES=""
for dev in /dev/neuron*; do
    if [ -e "$dev" ]; then
        NEURON_DEVICES="$NEURON_DEVICES --device=$dev"
        echo "  Found: $dev"
    fi
done

if [ -z "$NEURON_DEVICES" ]; then
    echo "‚ö†Ô∏è  Warning: No Neuron devices found. Model load test may fail."
    echo "  This is expected if not running on an Inferentia/Trainium instance."
fi

docker run --rm \
    $NEURON_DEVICES \
    -v /tmp/neuron-compile:/app \
    {{ container_image }} \
    /bin/bash -c "
python3 << 'LOADTEST'
import torch
import torch_neuronx
import json
import sys

print(f'Using torch-neuronx: {torch_neuronx.__version__}')

try:
    model_path = '/app/output/compiled_model.pt'
    print(f'Loading model from: {model_path}')
    
    compiled_model = torch.jit.load(model_path)
    
    result = {
        'status': 'COMPATIBLE',
        'message': 'Model compiled and loaded successfully',
        'torch_neuronx_version': torch_neuronx.__version__,
        'pytorch_version': torch.__version__,
        'environment': 'Docker Neuron DLC'
    }
    print(f\"‚úÖ {result['message']}\")
    
except Exception as e:
    result = {
        'status': 'ERROR',
        'error': str(e),
        'torch_neuronx_version': torch_neuronx.__version__,
        'pytorch_version': torch.__version__
    }
    print(f'‚ùå Load test failed: {e}')
    sys.exit(1)

# Save result
with open('/app/output/result.json', 'w') as f:
    json.dump(result, f, indent=2)
LOADTEST
" || {
    echo '{"status": "ERROR", "error": "Model load validation failed"}' > /tmp/neuron-compile/output/result.json
}

echo "‚úÖ Model validation complete"

# =============================================================================
# AUTOMATION: Create Jupyter Kernel for User Experience
# =============================================================================
echo ""
echo "ü§ñ Creating Jupyter Kernel for {{ neuron_sdk_version }}..."

# -----------------------------------------------------------------------------
# SETUP DOCKER JUPYTER KERNEL
# -----------------------------------------------------------------------------
# Since the SageMaker host (Amazon Linux 2) may not support the newer glibc
# required by PyTorch 2.9+, we register the DLC container itself as a kernel.
# This guarantees the environment matches exactly (PyTorch 2.9.0 + Neuron SDK).

KERNEL_DIR="/home/ec2-user/.local/share/jupyter/kernels/neuron_docker"
LAUNCHER_PATH="/home/ec2-user/start_neuron_kernel.sh"
mkdir -p "$KERNEL_DIR"

# 1. Create the Launcher Script
# This script is called by Jupyter. It runs the container and starts the kernel inside.
cat <<EOF > "$LAUNCHER_PATH"
#!/bin/bash
# Capture the connection file path passed by Jupyter
CONNECTION_FILE="\$1"

# Run the container with:
# - --net=host: To allow connection to Jupyter ports
# - -v: Mount the connection file directly (safer than directory mount)
# - -v: Mount the user's home directory so they can access their files
# - --cap-add=IPC_LOCK: Allow shared memory locking (required for ML workloads)
# - --privileged: Pass all neuron devices
# - --rm: Clean up after exit
# - -e PYTHONUNBUFFERED=1: Force Python to flush stdout/stderr (good for logging)
docker run --rm -i \\
    --net=host \\
    --pid=host \\
    --ipc=host \\
    --privileged \\
    --cap-add=IPC_LOCK \\
    -e PYTHONUNBUFFERED=1 \\
    -v "\$CONNECTION_FILE":"\$CONNECTION_FILE" \\
    -v "/home/ec2-user:/home/ec2-user" \\
    -w "/home/ec2-user" \\
    {{ container_image }} \\
    bash -c "echo 'DEBUG: Installing ipykernel...' >&2 && \
             python3 -m pip install ipykernel --break-system-packages >&2 && \
             echo 'DEBUG: Checking if ipykernel installed...' >&2 && \
             python3 -c 'import ipykernel; print(ipykernel.__file__)' >&2 && \
             echo 'DEBUG: Starting kernel...' >&2 && \
             python3 -m ipykernel -f \$CONNECTION_FILE"
EOF

chmod +x "$LAUNCHER_PATH"
chown ec2-user:ec2-user "$LAUNCHER_PATH"

# 2. Create the Kernel Spec
cat <<EOF > "$KERNEL_DIR/kernel.json"
{
 "argv": ["$LAUNCHER_PATH", "{connection_file}"],
 "display_name": "Neuron SDK {{ neuron_sdk_version }} (Docker)",
 "language": "python"
}
EOF

# Ensure permissions
chown -R ec2-user:ec2-user "/home/ec2-user/.local/share/jupyter"

echo "‚úÖ Registered Docker-based Jupyter Kernel: Neuron SDK {{ neuron_sdk_version }}"

# =============================================================================
# Upload Results
# =============================================================================
echo ""
echo "üì§ Uploading results to S3..."
aws s3 cp /tmp/neuron-compile/output/result.json "$OUTPUT_S3_URI/result.json"
if [ -f /tmp/neuron-compile/output/compiled_model.pt ]; then
    aws s3 cp /tmp/neuron-compile/output/compiled_model.pt "$OUTPUT_S3_URI/compiled_model.pt"
fi

# Upload compilation logs if they exist
if [ -f /tmp/neuron-compile/output/log-neuron-cc.txt ]; then
    aws s3 cp /tmp/neuron-compile/output/log-neuron-cc.txt "$OUTPUT_S3_URI/log-neuron-cc.txt"
fi

# =============================================================================
# Copy Compiled Model to Persistent Storage (for JupyterLab access)
# =============================================================================
echo ""
echo "üì¶ Copying compiled artifacts to persistent storage..."
PERSISTENT_DIR="/home/ec2-user/SageMaker/neuron-compiled-models"
mkdir -p "$PERSISTENT_DIR"
cp -r /tmp/neuron-compile/* "$PERSISTENT_DIR/"
chown -R ec2-user:ec2-user "$PERSISTENT_DIR"
echo "‚úÖ Artifacts available at: $PERSISTENT_DIR"

echo ""
echo "=============================================="
echo "‚úÖ NEURON VALIDATION COMPLETE"
echo "=============================================="
echo ""
echo "üìä Results uploaded to: $OUTPUT_S3_URI"
echo ""
echo "üéØ To use the compiled model:"
echo ""
echo "   Option 1: Use the Ready-Made Jupyter Kernel (Recommended)"
echo "   ---------------------------------------------------------------"
echo "   1. Open a new notebook in JupyterLab"
echo "   2. Select Kernel: \"Neuron SDK {{ neuron_sdk_version }} (Docker)\""
echo "   3. Run:"
echo "      import torch"
echo "      model = torch.jit.load('$PERSISTENT_DIR/output/compiled_model.pt')"
echo "      print('Model loaded successfully!')"
echo ""
echo "   Option 2: Run in Docker (Matches compilation environment exactly)"
echo "   ---------------------------------------------------------------"
echo "   docker run -it --rm -v $PERSISTENT_DIR:/app \\"
echo "     {{ container_image }} bash"
echo ""
echo "   Then in Docker:"
echo "   >>> python3"
echo "   >>> import torch"
echo "   >>> model = torch.jit.load('/app/output/compiled_model.pt')"
echo ""
echo "üìö Environment details:"
echo "   - Jupyter Kernel: $DISPLAY_NAME"
echo "   - Docker Image: {{ container_image }}"
echo "   - PyTorch: 2.9.0"
echo "   - Neuron SDK: {{ neuron_sdk_version }}"
echo ""