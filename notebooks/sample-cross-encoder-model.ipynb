{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c58c856-435d-446c-aaa6-1f11d0e70e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch: 2.9.0+cu128\n",
      "‚úÖ Neuron SDK: 2.9.0.2.11.19912+e48cd891\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ Neuron SDK: {torch_neuronx.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83bc1d25-3369-4af6-806e-9ba1fccbdcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found compilation artifacts at: /home/ec2-user/SageMaker/neuron-compiled-models\n",
      "\n",
      "üìÅ Compilation output directory:\n",
      "total 81M\n",
      "-rw-r--r-- 1 ubuntu ubuntu 81M Jan 20 02:39 compiled_model.pt\n",
      "-rw-r--r-- 1 ubuntu ubuntu 213 Jan 20 02:39 result.json\n",
      "\n",
      "üìÑ Original model files:\n",
      "total 88M\n",
      "-rw-r--r-- 1 ubuntu ubuntu  978 Jan 20 02:39 config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu  87M Jan 20 02:39 model.safetensors\n",
      "-rw-r--r-- 1 ubuntu ubuntu  695 Jan 20 02:39 special_tokens_map.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 695K Jan 20 02:39 tokenizer.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.5K Jan 20 02:39 tokenizer_config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 227K Jan 20 02:39 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The lifecycle script copies artifacts here so they are visible inside the Docker kernel\n",
    "artifact_path = '/home/ec2-user/SageMaker/neuron-compiled-models'\n",
    "\n",
    "if os.path.exists(artifact_path):\n",
    "    print(f\"\\n‚úÖ Found compilation artifacts at: {artifact_path}\")\n",
    "    \n",
    "    print(\"\\nüìÅ Compilation output directory:\")\n",
    "    !ls -lh {artifact_path}/output/\n",
    "    \n",
    "    print(\"\\nüìÑ Original model files:\")\n",
    "    !ls -lh {artifact_path}/model/\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Artifacts not found at {artifact_path}\")\n",
    "    print(\"If the lifecycle script is still running, wait a few minutes and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0b5adf0-4ce5-430c-b145-140d5152a89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Compilation Result:\n",
      "{\n",
      "  \"status\": \"COMPATIBLE\",\n",
      "  \"message\": \"Model compiled and loaded successfully\",\n",
      "  \"torch_neuronx_version\": \"2.9.0.2.11.19912+e48cd891\",\n",
      "  \"pytorch_version\": \"2.9.0+cu128\",\n",
      "  \"environment\": \"Docker Neuron DLC\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path to the persistent artifact directory\n",
    "result_path = '/home/ec2-user/SageMaker/neuron-compiled-models/output/result.json'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(result_path):\n",
    "        with open(result_path, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        print(\"üìä Compilation Result:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Result file not found at: {result_path}\")\n",
    "        print(\"The lifecycle script may still be running, or compilation failed.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading result: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be8966ca-4dab-45cc-99c4-157387bb3b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Your model compiled successfully!\n",
      "üíæ Compiled Model: /home/ec2-user/SageMaker/neuron-compiled-models/output/compiled_model.pt\n",
      "\n",
      "Model compiled and loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "result_path = '/home/ec2-user/SageMaker/neuron-compiled-models/output/result.json'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(result_path):\n",
    "        with open(result_path, 'r') as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        if result.get('status') == 'COMPATIBLE':\n",
    "            print(\"\\n‚úÖ Your model compiled successfully!\")\n",
    "            \n",
    "            if 'input_shape' in result:\n",
    "                print(f\"üìê Input Shape: {result['input_shape']}\")\n",
    "            if 'model_type' in result:\n",
    "                print(f\"ü§ñ Model Type: {result['model_type']}\")\n",
    "            if 'detected_architecture' in result:\n",
    "                print(f\"üèóÔ∏è  Architecture: {result['detected_architecture']}\")\n",
    "            \n",
    "            # The output path in result.json might refer to the compile-time path\n",
    "            # We print the path accessible to this notebook\n",
    "            model_path = '/home/ec2-user/SageMaker/neuron-compiled-models/output/compiled_model.pt'\n",
    "            print(f\"üíæ Compiled Model: {model_path}\")\n",
    "            \n",
    "            print(f\"\\n{result.get('message', 'Compilation completed')}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Compilation status: {result.get('status')}\")\n",
    "            # Check various error fields\n",
    "            error = result.get('error') or result.get('error_message') or result.get('message', 'N/A')\n",
    "            print(f\"Error: {error}\")\n",
    "            print(\"\\n‚ö†Ô∏è  Stop here - compilation failed.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Result file not found at: {result_path}\")\n",
    "        print(\"Compilation may still be running. Wait a few minutes and re-run this cell.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99a40607-b2fc-4ba2-9c54-841c50a80b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Artifacts directory: /home/ec2-user/SageMaker/neuron-compiled-models\n",
      "\n",
      "üîÑ Loading compiled model from: /home/ec2-user/SageMaker/neuron-compiled-models/output/compiled_model.pt...\n",
      "‚úÖ Success! Neuron model loaded.\n",
      "\n",
      "üîÑ Loading tokenizer from: /home/ec2-user/SageMaker/neuron-compiled-models/model...\n",
      "‚úÖ Success! Tokenizer loaded.\n",
      "\n",
      "üöÄ Ready for inference! Run the next cell to test.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Optional: Silence tqdm warnings\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "# The standard path where the lifecycle script persists artifacts\n",
    "# This path is visible both in the File Browser and inside this Docker kernel\n",
    "base_path = '/home/ec2-user/SageMaker/neuron-compiled-models'\n",
    "model_path = os.path.join(base_path, 'output', 'compiled_model.pt')\n",
    "tokenizer_path = os.path.join(base_path, 'model')\n",
    "\n",
    "print(f\"üìÇ Artifacts directory: {base_path}\")\n",
    "\n",
    "# 1. Load the Compiled Model\n",
    "print(f\"\\nüîÑ Loading compiled model from: {model_path}...\")\n",
    "try:\n",
    "    if not os.path.exists(model_path):\n",
    "        raise FileNotFoundError(f\"Model file not found at {model_path}\")\n",
    "        \n",
    "    compiled_model = torch.jit.load(model_path)\n",
    "    print(\"‚úÖ Success! Neuron model loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load model: {e}\")\n",
    "    # We continue to try loading the tokenizer for debugging purposes\n",
    "\n",
    "# 2. Load the Tokenizer\n",
    "print(f\"\\nüîÑ Loading tokenizer from: {tokenizer_path}...\")\n",
    "try:\n",
    "    if not os.path.exists(tokenizer_path):\n",
    "        raise FileNotFoundError(f\"Tokenizer directory not found at {tokenizer_path}\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    print(\"‚úÖ Success! Tokenizer loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load tokenizer: {e}\")\n",
    "\n",
    "# 3. Final Status\n",
    "if 'compiled_model' in locals() and 'tokenizer' in locals():\n",
    "    print(\"\\nüöÄ Ready for inference! Run the next cell to test.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Setup incomplete. Check errors above.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c86227bc-38e9-4209-9d2f-774c61cd09b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 25])\n",
      "Attention mask shape: torch.Size([1, 25])\n"
     ]
    }
   ],
   "source": [
    "# Test inference with example input\n",
    "query = \"What is machine learning?\"\n",
    "document = \"Machine learning is a subset of artificial intelligence that uses algorithms to learn patterns from data.\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(\n",
    "    query,\n",
    "    document,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7d4fa7-b9a7-4661-bc56-7d7553cfe098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([1, 512])\n",
      "\n",
      "üîç Functional Test...\n",
      "‚úÖ Inference successful with (input_ids, attention_mask)!\n",
      "\n",
      "‚è±Ô∏è  Running Latency Benchmark (100 runs)...\n",
      "  Avg Latency: 2.19 ms\n",
      "  P99 Latency: 2.24 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Prepare Input (CRITICAL: Must match compiled shape)\n",
    "# =============================================================================\n",
    "query = \"What is machine learning?\"\n",
    "document = \"Machine learning is a subset of artificial intelligence that uses algorithms to learn patterns from data.\"\n",
    "\n",
    "# Force padding to 512 to match the compiled model\n",
    "inputs = tokenizer(\n",
    "    query,\n",
    "    document,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=512,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\"  # <--- THIS IS THE FIX\n",
    ")\n",
    "\n",
    "print(f\"Input Shape: {inputs['input_ids'].shape}\")\n",
    "# Prepare tuple calling convention\n",
    "# Depending on how compile_script.py traced it, it might expect\n",
    "# (input_ids,) OR (input_ids, attention_mask).\n",
    "# Try passing both first, which is standard for BERT-like models.\n",
    "example_inputs = (inputs['input_ids'], inputs['attention_mask'])\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Functional Test\n",
    "# =============================================================================\n",
    "print(\"\\nüîç Functional Test...\")\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        # Try passing both inputs\n",
    "        outputs = compiled_model(*example_inputs)\n",
    "        print(\"‚úÖ Inference successful with (input_ids, attention_mask)!\")\n",
    "except RuntimeError as e:\n",
    "    if \"expected\" in str(e):\n",
    "        print(f\"‚ö†Ô∏è  Shape/Arg mismatch: {e}\")\n",
    "        print(\"Retrying with just input_ids...\")\n",
    "        try:\n",
    "            # Fallback: maybe it was compiled with only input_ids?\n",
    "            outputs = compiled_model(inputs['input_ids'])\n",
    "            example_inputs = (inputs['input_ids'],) # Update for benchmark\n",
    "            print(\"‚úÖ Inference successful with (input_ids) only!\")\n",
    "        except Exception as e2:\n",
    "            print(f\"‚ùå Failed again: {e2}\")\n",
    "            raise e\n",
    "    else:\n",
    "        print(f\"‚ùå Inference failed: {e}\")\n",
    "        raise e\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Latency Benchmark\n",
    "# =============================================================================\n",
    "print(\"\\n‚è±Ô∏è  Running Latency Benchmark (100 runs)...\")\n",
    "\n",
    "# Warmup\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        _ = compiled_model(*example_inputs)\n",
    "\n",
    "# Measure\n",
    "latencies = []\n",
    "for _ in range(100):\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = compiled_model(*example_inputs)\n",
    "    latencies.append((time.time() - start) * 1000)\n",
    "\n",
    "print(f\"  Avg Latency: {np.mean(latencies):.2f} ms\")\n",
    "print(f\"  P99 Latency: {np.percentile(latencies, 99):.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f455e-18fa-4ae8-bfda-e66bdd42a25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neuron SDK 2.27.1 (Docker)",
   "language": "python",
   "name": "neuron_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
