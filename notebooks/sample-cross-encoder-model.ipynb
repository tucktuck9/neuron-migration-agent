{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b955a123-5d62-4836-9a42-3a6dbbbb7a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch: 2.9.0+cu128\n",
      "‚úÖ Neuron SDK: 2.9.0.2.11.19912+e48cd891\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "print(f\"‚úÖ Neuron SDK: {torch_neuronx.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec4fd621-83e6-4a29-b8c4-01a4ecaa8040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Found compilation artifacts at: /home/ec2-user/SageMaker/neuron-compiled-models\n",
      "\n",
      "üìÅ Compilation output directory:\n",
      "total 81M\n",
      "-rw-r--r-- 1 ubuntu ubuntu 81M Jan 20 00:30 compiled_model.pt\n",
      "-rw-r--r-- 1 ubuntu ubuntu 213 Jan 20 00:30 result.json\n",
      "\n",
      "üìÑ Original model files:\n",
      "total 88M\n",
      "-rw-r--r-- 1 ubuntu ubuntu  978 Jan 20 00:30 config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu  87M Jan 20 00:30 model.safetensors\n",
      "-rw-r--r-- 1 ubuntu ubuntu  695 Jan 20 00:30 special_tokens_map.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 695K Jan 20 00:30 tokenizer.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 1.5K Jan 20 00:30 tokenizer_config.json\n",
      "-rw-r--r-- 1 ubuntu ubuntu 227K Jan 20 00:30 vocab.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The lifecycle script copies artifacts here so they are visible inside the Docker kernel\n",
    "artifact_path = '/home/ec2-user/SageMaker/neuron-compiled-models'\n",
    "\n",
    "if os.path.exists(artifact_path):\n",
    "    print(f\"\\n‚úÖ Found compilation artifacts at: {artifact_path}\")\n",
    "    \n",
    "    print(\"\\nüìÅ Compilation output directory:\")\n",
    "    !ls -lh {artifact_path}/output/\n",
    "    \n",
    "    print(\"\\nüìÑ Original model files:\")\n",
    "    !ls -lh {artifact_path}/model/\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Artifacts not found at {artifact_path}\")\n",
    "    print(\"If the lifecycle script is still running, wait a few minutes and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e12e1437-9445-4f33-808c-d233dc1228cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Compilation Result:\n",
      "{\n",
      "  \"status\": \"COMPATIBLE\",\n",
      "  \"message\": \"Model compiled and loaded successfully\",\n",
      "  \"torch_neuronx_version\": \"2.9.0.2.11.19912+e48cd891\",\n",
      "  \"pytorch_version\": \"2.9.0+cu128\",\n",
      "  \"environment\": \"Docker Neuron DLC\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Path to the persistent artifact directory\n",
    "result_path = '/home/ec2-user/SageMaker/neuron-compiled-models/output/result.json'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(result_path):\n",
    "        with open(result_path, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        print(\"üìä Compilation Result:\")\n",
    "        print(json.dumps(result, indent=2))\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Result file not found at: {result_path}\")\n",
    "        print(\"The lifecycle script may still be running, or compilation failed.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading result: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36392bae-cdd3-4661-820b-901abf88d603",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Your model compiled successfully!\n",
      "üíæ Compiled Model: /home/ec2-user/SageMaker/neuron-compiled-models/output/compiled_model.pt\n",
      "\n",
      "Model compiled and loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "result_path = '/home/ec2-user/SageMaker/neuron-compiled-models/output/result.json'\n",
    "\n",
    "try:\n",
    "    if os.path.exists(result_path):\n",
    "        with open(result_path, 'r') as f:\n",
    "            result = json.load(f)\n",
    "\n",
    "        if result.get('status') == 'COMPATIBLE':\n",
    "            print(\"\\n‚úÖ Your model compiled successfully!\")\n",
    "            \n",
    "            if 'input_shape' in result:\n",
    "                print(f\"üìê Input Shape: {result['input_shape']}\")\n",
    "            if 'model_type' in result:\n",
    "                print(f\"ü§ñ Model Type: {result['model_type']}\")\n",
    "            if 'detected_architecture' in result:\n",
    "                print(f\"üèóÔ∏è  Architecture: {result['detected_architecture']}\")\n",
    "            \n",
    "            # The output path in result.json might refer to the compile-time path\n",
    "            # We print the path accessible to this notebook\n",
    "            model_path = '/home/ec2-user/SageMaker/neuron-compiled-models/output/compiled_model.pt'\n",
    "            print(f\"üíæ Compiled Model: {model_path}\")\n",
    "            \n",
    "            print(f\"\\n{result.get('message', 'Compilation completed')}\")\n",
    "        else:\n",
    "            print(f\"\\n‚ùå Compilation status: {result.get('status')}\")\n",
    "            # Check various error fields\n",
    "            error = result.get('error') or result.get('error_message') or result.get('message', 'N/A')\n",
    "            print(f\"Error: {error}\")\n",
    "            print(\"\\n‚ö†Ô∏è  Stop here - compilation failed.\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Result file not found at: {result_path}\")\n",
    "        print(\"Compilation may still be running. Wait a few minutes and re-run this cell.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b5116fc-6337-4238-a6e8-eb5dc452c346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading compiled model from /home/ec2-user/SageMaker/neuron-compiled-models/output/compiled_model.pt...\n",
      "‚úÖ Compiled model loaded successfully!\n",
      "Loading tokenizer from /home/ec2-user/SageMaker/neuron-compiled-models/model/...\n",
      "‚úÖ Tokenizer loaded successfully!\n",
      "\n",
      "‚úÖ Model and Tokenizer ready for inference on Neuron!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_neuronx\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "\n",
    "# Path where artifacts are persisted\n",
    "base_path = '/home/ec2-user/SageMaker/neuron-compiled-models'\n",
    "model_path = f'{base_path}/output/compiled_model.pt'\n",
    "tokenizer_path = f'{base_path}/model/'\n",
    "\n",
    "print(f\"Loading compiled model from {model_path}...\")\n",
    "\n",
    "# Load the Neuron-compiled model\n",
    "try:\n",
    "    compiled_model = torch.jit.load(model_path)\n",
    "    print(\"‚úÖ Compiled model loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load compiled model: {e}\")\n",
    "    # Don't raise immediately, let's try loading tokenizer too for debugging\n",
    "    pass\n",
    "\n",
    "# Load tokenizer from original model files\n",
    "print(f\"Loading tokenizer from {tokenizer_path}...\")\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "    print(\"‚úÖ Tokenizer loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to load tokenizer: {e}\")\n",
    "    \n",
    "if 'compiled_model' in locals() and 'tokenizer' in locals():\n",
    "    print(\"\\n‚úÖ Model and Tokenizer ready for inference on Neuron!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4c853-60b2-42f7-a9ba-8dc242642e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neuron SDK 2.27.1 (Docker)",
   "language": "python",
   "name": "neuron_docker"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
